{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ü§ñ Blockchain Fraud Detection - Machine Learning Experiments\n",
        "\n",
        "This notebook experiments with machine learning models for detecting fraudulent wallet behavior on the Ethereum blockchain.\n",
        "\n",
        "## Objectives\n",
        "1. **Feature Engineering**: Compute wallet-level features from transaction data\n",
        "2. **Model Training**: Train anomaly detection models (Isolation Forest, LOF, DBSCAN)\n",
        "3. **Evaluation**: Analyze model performance and interpret results\n",
        "4. **Save Results**: Store features and scores to BigQuery for production use\n",
        "\n",
        "## Models Used\n",
        "- **Isolation Forest**: Tree-based anomaly detection\n",
        "- **Local Outlier Factor (LOF)**: Density-based outlier detection\n",
        "- **DBSCAN**: Clustering-based anomaly detection\n",
        "- **Ensemble**: Weighted combination of all models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "IndentationError",
          "evalue": "unexpected indent (4232343709.py, line 4)",
          "output_type": "error",
          "traceback": [
            "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mget_ipython().run_line_magic('pip', 'install google-cloud-bigquery pandas numpy scikit-learn matplotlib seaborn db-dtypes pyarrow python-dotenv')\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unexpected indent\n"
          ]
        }
      ],
      "source": [
        "# Install required packages (run once if needed)\n",
        "# Use %pip for better Jupyter compatibility, especially on Windows\n",
        "# Uncomment the line below to install packages\n",
        "# %pip install google-cloud-bigquery pandas numpy scikit-learn matplotlib seaborn db-dtypes pyarrow python-dotenv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "from typing import Tuple, Dict, List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "from google.cloud import bigquery\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configure display\n",
        "pd.set_option('display.max_columns', 50)\n",
        "\n",
        "# Matplotlib style - use fallback for compatibility\n",
        "try:\n",
        "    plt.style.use('seaborn-v0_8-darkgrid')\n",
        "except OSError:\n",
        "    try:\n",
        "        plt.style.use('seaborn-darkgrid')\n",
        "    except OSError:\n",
        "        plt.style.use('ggplot')  # Fallback to ggplot\n",
        "        \n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load environment and configure BigQuery\n",
        "load_dotenv()\n",
        "\n",
        "PROJECT_ID = os.getenv('GOOGLE_CLOUD_PROJECT', 'blockchain-481614')\n",
        "RAW_DATASET = 'blockchain_raw'\n",
        "ML_DATASET = 'blockchain_ml'\n",
        "\n",
        "# Initialize BigQuery client\n",
        "client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "def run_query(query: str) -> pd.DataFrame:\n",
        "    \"\"\"Execute BigQuery query and return DataFrame.\"\"\"\n",
        "    return client.query(query).to_dataframe()\n",
        "\n",
        "print(f\"‚úÖ Project: {PROJECT_ID}\")\n",
        "print(f\"‚úÖ Raw Dataset: {RAW_DATASET}\")\n",
        "print(f\"‚úÖ ML Dataset: {ML_DATASET}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Feature Engineering\n",
        "\n",
        "Compute wallet-level features from raw transaction data. These features capture various aspects of wallet behavior that may indicate fraudulent activity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature engineering SQL query\n",
        "# This computes wallet-level features from transaction data\n",
        "\n",
        "features_query = f\"\"\"\n",
        "WITH wallet_transactions AS (\n",
        "    -- Get all transactions with wallet as sender\n",
        "    SELECT\n",
        "        from_address AS wallet_address,\n",
        "        CAST(value_eth AS FLOAT64) as value_eth,\n",
        "        CAST(gas_price AS FLOAT64) as gas_price,\n",
        "        CAST(gas_used AS INT64) as gas_used,\n",
        "        to_address AS counterparty,\n",
        "        transaction_timestamp,\n",
        "        'out' AS direction\n",
        "    FROM `{PROJECT_ID}.{RAW_DATASET}.raw_transactions`\n",
        "    WHERE from_address IS NOT NULL\n",
        "    \n",
        "    UNION ALL\n",
        "    \n",
        "    -- Get all transactions with wallet as receiver\n",
        "    SELECT\n",
        "        to_address AS wallet_address,\n",
        "        CAST(value_eth AS FLOAT64) as value_eth,\n",
        "        CAST(gas_price AS FLOAT64) as gas_price,\n",
        "        CAST(gas_used AS INT64) as gas_used,\n",
        "        from_address AS counterparty,\n",
        "        transaction_timestamp,\n",
        "        'in' AS direction\n",
        "    FROM `{PROJECT_ID}.{RAW_DATASET}.raw_transactions`\n",
        "    WHERE to_address IS NOT NULL\n",
        "),\n",
        "\n",
        "basic_features AS (\n",
        "    SELECT\n",
        "        wallet_address,\n",
        "        \n",
        "        -- Transaction counts\n",
        "        COUNT(*) AS tx_count,\n",
        "        COUNTIF(direction = 'in') AS tx_count_in,\n",
        "        COUNTIF(direction = 'out') AS tx_count_out,\n",
        "        \n",
        "        -- Value statistics\n",
        "        SUM(value_eth) AS total_value,\n",
        "        SUM(CASE WHEN direction = 'in' THEN value_eth ELSE 0 END) AS total_value_in,\n",
        "        SUM(CASE WHEN direction = 'out' THEN value_eth ELSE 0 END) AS total_value_out,\n",
        "        AVG(value_eth) AS avg_value,\n",
        "        STDDEV(value_eth) AS std_value,\n",
        "        MIN(value_eth) AS min_value,\n",
        "        MAX(value_eth) AS max_value,\n",
        "        \n",
        "        -- Counterparty analysis\n",
        "        COUNT(DISTINCT counterparty) AS unique_counterparties,\n",
        "        \n",
        "        -- Gas statistics\n",
        "        AVG(gas_used) AS avg_gas_used,\n",
        "        AVG(gas_price / 1e9) AS avg_gas_price_gwei,\n",
        "        \n",
        "        -- Temporal\n",
        "        MIN(transaction_timestamp) AS first_tx_time,\n",
        "        MAX(transaction_timestamp) AS last_tx_time,\n",
        "        TIMESTAMP_DIFF(MAX(transaction_timestamp), MIN(transaction_timestamp), DAY) AS activity_span_days,\n",
        "        COUNT(DISTINCT DATE(transaction_timestamp)) AS active_days\n",
        "        \n",
        "    FROM wallet_transactions\n",
        "    GROUP BY wallet_address\n",
        "    HAVING COUNT(*) >= 2  -- Minimum 2 transactions\n",
        ")\n",
        "\n",
        "SELECT\n",
        "    wallet_address,\n",
        "    tx_count,\n",
        "    tx_count_in,\n",
        "    tx_count_out,\n",
        "    total_value,\n",
        "    total_value_in,\n",
        "    total_value_out,\n",
        "    avg_value,\n",
        "    COALESCE(std_value, 0) as std_value,\n",
        "    min_value,\n",
        "    max_value,\n",
        "    unique_counterparties,\n",
        "    avg_gas_used,\n",
        "    avg_gas_price_gwei,\n",
        "    activity_span_days,\n",
        "    active_days,\n",
        "    \n",
        "    -- Derived features\n",
        "    SAFE_DIVIDE(tx_count_in, tx_count_out) AS in_out_ratio,\n",
        "    total_value_in - total_value_out AS net_flow,\n",
        "    SAFE_DIVIDE(tx_count, GREATEST(active_days, 1)) AS tx_per_active_day,\n",
        "    SAFE_DIVIDE(total_value, tx_count) AS value_per_tx\n",
        "    \n",
        "FROM basic_features\n",
        "WHERE wallet_address IS NOT NULL\n",
        "\"\"\"\n",
        "\n",
        "print(\"Running feature engineering query...\")\n",
        "features_df = run_query(features_query)\n",
        "print(f\"‚úÖ Computed features for {len(features_df):,} wallets\")\n",
        "print(f\"‚úÖ Total features: {len(features_df.columns) - 1}\")  # -1 for wallet_address\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explore the features\n",
        "print(\"Feature Columns:\")\n",
        "print(list(features_df.columns))\n",
        "print(\"\\nSample Data:\")\n",
        "display(features_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature statistics\n",
        "print(\"üìä Feature Statistics:\")\n",
        "display(features_df.describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Preparation\n",
        "\n",
        "Prepare the data for machine learning by handling missing values and scaling features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare features for ML\n",
        "# Get numeric columns only (exclude wallet_address)\n",
        "feature_cols = [col for col in features_df.columns if col != 'wallet_address']\n",
        "X = features_df[feature_cols].copy()\n",
        "\n",
        "# Handle missing and infinite values\n",
        "X = X.replace([np.inf, -np.inf], np.nan)\n",
        "X = X.fillna(0)\n",
        "\n",
        "print(f\"Feature matrix shape: {X.shape}\")\n",
        "print(f\"Missing values: {X.isna().sum().sum()}\")\n",
        "\n",
        "# Store wallet addresses for later\n",
        "wallet_addresses = features_df['wallet_address'].values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scale features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(f\"Scaled feature matrix shape: {X_scaled.shape}\")\n",
        "print(f\"Feature means (should be ~0): {X_scaled.mean(axis=0)[:5].round(4)}\")\n",
        "print(f\"Feature stds (should be ~1): {X_scaled.std(axis=0)[:5].round(4)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Training\n",
        "\n",
        "Train multiple anomaly detection models to identify potentially fraudulent wallets.\n",
        "\n",
        "### 4.1 Isolation Forest\n",
        "Isolation Forest isolates anomalies by randomly selecting features and split values. Anomalies are easier to isolate and thus have shorter path lengths.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Isolation Forest\n",
        "print(\"Training Isolation Forest...\")\n",
        "\n",
        "iso_forest = IsolationForest(\n",
        "    n_estimators=100,\n",
        "    contamination=0.1,  # Expected 10% anomalies\n",
        "    max_samples='auto',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit and predict\n",
        "iso_labels = iso_forest.fit_predict(X_scaled)\n",
        "iso_scores = iso_forest.decision_function(X_scaled)\n",
        "\n",
        "# Convert labels: -1 (anomaly) -> 1, 1 (normal) -> 0\n",
        "iso_anomaly = (iso_labels == -1).astype(int)\n",
        "\n",
        "print(f\"‚úÖ Isolation Forest trained\")\n",
        "print(f\"   Anomalies detected: {iso_anomaly.sum():,} ({iso_anomaly.mean()*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Local Outlier Factor (LOF)\n",
        "LOF measures the local density deviation of a data point with respect to its neighbors. Points with substantially lower density than their neighbors are considered outliers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Local Outlier Factor\n",
        "print(\"Training Local Outlier Factor...\")\n",
        "\n",
        "lof = LocalOutlierFactor(\n",
        "    n_neighbors=20,\n",
        "    contamination=0.1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit and predict\n",
        "lof_labels = lof.fit_predict(X_scaled)\n",
        "lof_scores = -lof.negative_outlier_factor_  # Negate to make higher = more anomalous\n",
        "\n",
        "# Convert labels\n",
        "lof_anomaly = (lof_labels == -1).astype(int)\n",
        "\n",
        "print(f\"‚úÖ LOF trained\")\n",
        "print(f\"   Anomalies detected: {lof_anomaly.sum():,} ({lof_anomaly.mean()*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 DBSCAN\n",
        "DBSCAN clusters data points based on density. Points that don't belong to any cluster (noise points) are considered anomalies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train DBSCAN\n",
        "print(\"Training DBSCAN...\")\n",
        "\n",
        "dbscan = DBSCAN(\n",
        "    eps=0.5,\n",
        "    min_samples=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit and predict\n",
        "dbscan_labels = dbscan.fit_predict(X_scaled)\n",
        "\n",
        "# Noise points (label -1) are anomalies\n",
        "dbscan_anomaly = (dbscan_labels == -1).astype(int)\n",
        "\n",
        "# Calculate silhouette score (excluding noise)\n",
        "non_noise_mask = dbscan_labels != -1\n",
        "if len(np.unique(dbscan_labels[non_noise_mask])) > 1:\n",
        "    sil_score = silhouette_score(X_scaled[non_noise_mask], dbscan_labels[non_noise_mask])\n",
        "else:\n",
        "    sil_score = 0\n",
        "\n",
        "print(f\"‚úÖ DBSCAN trained\")\n",
        "print(f\"   Clusters found: {len(np.unique(dbscan_labels)) - 1}\")  # -1 for noise\n",
        "print(f\"   Noise points (anomalies): {dbscan_anomaly.sum():,} ({dbscan_anomaly.mean()*100:.1f}%)\")\n",
        "print(f\"   Silhouette Score: {sil_score:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.4 Ensemble Model\n",
        "Combine all models into an ensemble score for more robust fraud detection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create ensemble fraud score\n",
        "def normalize_scores(scores):\n",
        "    \"\"\"Normalize scores to 0-1 range.\"\"\"\n",
        "    min_s = scores.min()\n",
        "    max_s = scores.max()\n",
        "    if max_s == min_s:\n",
        "        return np.zeros_like(scores)\n",
        "    return (scores - min_s) / (max_s - min_s)\n",
        "\n",
        "# Normalize individual scores\n",
        "iso_norm = normalize_scores(-iso_scores)  # Negate because lower = more anomalous\n",
        "lof_norm = normalize_scores(lof_scores)\n",
        "dbscan_norm = dbscan_anomaly.astype(float)\n",
        "\n",
        "# Weighted ensemble (higher weight for Isolation Forest)\n",
        "weights = {'isolation_forest': 0.4, 'lof': 0.35, 'dbscan': 0.25}\n",
        "ensemble_score = (\n",
        "    weights['isolation_forest'] * iso_norm +\n",
        "    weights['lof'] * lof_norm +\n",
        "    weights['dbscan'] * dbscan_norm\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Ensemble scores computed\")\n",
        "print(f\"   Score range: {ensemble_score.min():.3f} - {ensemble_score.max():.3f}\")\n",
        "print(f\"   Mean score: {ensemble_score.mean():.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Evaluation and Visualization\n",
        "\n",
        "Analyze the model results and visualize the fraud score distribution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assign risk categories\n",
        "def get_risk_category(score):\n",
        "    \"\"\"Assign risk category based on fraud score.\"\"\"\n",
        "    if score >= 0.9:\n",
        "        return 'critical'\n",
        "    elif score >= 0.7:\n",
        "        return 'high'\n",
        "    elif score >= 0.4:\n",
        "        return 'medium'\n",
        "    else:\n",
        "        return 'low'\n",
        "\n",
        "risk_categories = np.array([get_risk_category(s) for s in ensemble_score])\n",
        "\n",
        "# Count by category\n",
        "from collections import Counter\n",
        "risk_counts = Counter(risk_categories)\n",
        "print(\"üìä Risk Category Distribution:\")\n",
        "for cat in ['low', 'medium', 'high', 'critical']:\n",
        "    count = risk_counts.get(cat, 0)\n",
        "    pct = count / len(risk_categories) * 100\n",
        "    print(f\"   {cat.capitalize()}: {count:,} ({pct:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize fraud score distribution\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# 1. Ensemble score histogram\n",
        "ax1 = axes[0, 0]\n",
        "ax1.hist(ensemble_score, bins=50, edgecolor='black', alpha=0.7, color='#e74c3c')\n",
        "ax1.axvline(x=0.4, color='orange', linestyle='--', linewidth=2, label='Medium Risk (0.4)')\n",
        "ax1.axvline(x=0.7, color='red', linestyle='--', linewidth=2, label='High Risk (0.7)')\n",
        "ax1.axvline(x=0.9, color='purple', linestyle='--', linewidth=2, label='Critical (0.9)')\n",
        "ax1.set_xlabel('Ensemble Fraud Score')\n",
        "ax1.set_ylabel('Frequency')\n",
        "ax1.set_title('Ensemble Fraud Score Distribution')\n",
        "ax1.legend()\n",
        "\n",
        "# 2. Individual model comparison\n",
        "ax2 = axes[0, 1]\n",
        "ax2.boxplot([iso_norm, lof_norm, dbscan_norm], labels=['Isolation Forest', 'LOF', 'DBSCAN'])\n",
        "ax2.set_ylabel('Normalized Score')\n",
        "ax2.set_title('Model Score Comparison')\n",
        "\n",
        "# 3. Risk category pie chart\n",
        "ax3 = axes[1, 0]\n",
        "colors_map = {'low': '#2ecc71', 'medium': '#f39c12', 'high': '#e74c3c', 'critical': '#8e44ad'}\n",
        "categories = ['low', 'medium', 'high', 'critical']\n",
        "counts = [risk_counts.get(c, 0) for c in categories]\n",
        "colors = [colors_map[c] for c in categories]\n",
        "ax3.pie(counts, labels=categories, autopct='%1.1f%%', colors=colors)\n",
        "ax3.set_title('Risk Category Distribution')\n",
        "\n",
        "# 4. Score percentiles\n",
        "ax4 = axes[1, 1]\n",
        "percentiles = [10, 25, 50, 75, 90, 95, 99]\n",
        "pct_values = [np.percentile(ensemble_score, p) for p in percentiles]\n",
        "ax4.bar([str(p) + '%' for p in percentiles], pct_values, color='#3498db', alpha=0.8)\n",
        "ax4.set_xlabel('Percentile')\n",
        "ax4.set_ylabel('Fraud Score')\n",
        "ax4.set_title('Fraud Score Percentiles')\n",
        "ax4.axhline(y=0.7, color='red', linestyle='--', label='High Risk Threshold')\n",
        "ax4.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('ml_fraud_detection_results.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show top suspicious wallets\n",
        "results_df = pd.DataFrame({\n",
        "    'wallet_address': wallet_addresses,\n",
        "    'fraud_score': ensemble_score,\n",
        "    'risk_category': risk_categories,\n",
        "    'isolation_forest_score': iso_norm,\n",
        "    'lof_score': lof_norm,\n",
        "    'dbscan_is_noise': dbscan_anomaly\n",
        "})\n",
        "\n",
        "# Merge with original features\n",
        "results_df = results_df.merge(features_df, on='wallet_address', how='left')\n",
        "\n",
        "# Top high-risk wallets\n",
        "print(\"üö® Top 10 High-Risk Wallets:\")\n",
        "high_risk_df = results_df[results_df['risk_category'].isin(['high', 'critical'])].sort_values('fraud_score', ascending=False)\n",
        "display(high_risk_df[['wallet_address', 'fraud_score', 'risk_category', 'tx_count', 'total_value']].head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Feature Importance Analysis\n",
        "\n",
        "Analyze which features contribute most to anomaly detection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare high-risk vs normal wallet features\n",
        "high_risk_wallets = results_df[results_df['risk_category'].isin(['high', 'critical'])]\n",
        "normal_wallets = results_df[results_df['risk_category'] == 'low']\n",
        "\n",
        "print(\"üìä Feature Comparison: High-Risk vs Normal Wallets\\n\")\n",
        "comparison_features = ['tx_count', 'total_value', 'avg_value', 'unique_counterparties', \n",
        "                       'activity_span_days', 'tx_per_active_day']\n",
        "\n",
        "for feat in comparison_features:\n",
        "    if feat in results_df.columns:\n",
        "        hr_mean = high_risk_wallets[feat].mean()\n",
        "        nr_mean = normal_wallets[feat].mean()\n",
        "        ratio = hr_mean / nr_mean if nr_mean > 0 else float('inf')\n",
        "        print(f\"{feat}:\")\n",
        "        print(f\"   High-Risk Mean: {hr_mean:,.2f}\")\n",
        "        print(f\"   Normal Mean: {nr_mean:,.2f}\")\n",
        "        print(f\"   Ratio: {ratio:.2f}x\")\n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Save Results to BigQuery\n",
        "\n",
        "Save the computed features and fraud scores to BigQuery for use by the production system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for BigQuery upload\n",
        "# Features table\n",
        "features_to_save = features_df.copy()\n",
        "features_to_save['computed_at'] = datetime.utcnow()\n",
        "\n",
        "print(f\"Features to save: {len(features_to_save):,} rows\")\n",
        "\n",
        "# Fraud scores table\n",
        "scores_to_save = pd.DataFrame({\n",
        "    'wallet_address': wallet_addresses,\n",
        "    'fraud_score': ensemble_score,\n",
        "    'risk_category': risk_categories,\n",
        "    'isolation_forest_score': iso_norm,\n",
        "    'lof_score': lof_norm,\n",
        "    'dbscan_is_noise': dbscan_anomaly.astype(bool),\n",
        "    'scored_at': datetime.utcnow()\n",
        "})\n",
        "\n",
        "print(f\"Scores to save: {len(scores_to_save):,} rows\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to save DataFrame to BigQuery\n",
        "def save_to_bigquery(df: pd.DataFrame, dataset: str, table: str, write_mode: str = 'WRITE_TRUNCATE'):\n",
        "    \"\"\"Save DataFrame to BigQuery table.\"\"\"\n",
        "    table_id = f\"{PROJECT_ID}.{dataset}.{table}\"\n",
        "    \n",
        "    # Create dataset if it doesn't exist\n",
        "    dataset_ref = f\"{PROJECT_ID}.{dataset}\"\n",
        "    try:\n",
        "        client.get_dataset(dataset_ref)\n",
        "    except Exception:\n",
        "        ds = bigquery.Dataset(dataset_ref)\n",
        "        ds.location = \"US\"\n",
        "        client.create_dataset(ds)\n",
        "        print(f\"Created dataset: {dataset_ref}\")\n",
        "    \n",
        "    # Configure job\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        write_disposition=write_mode\n",
        "    )\n",
        "    \n",
        "    # Upload\n",
        "    job = client.load_table_from_dataframe(df, table_id, job_config=job_config)\n",
        "    job.result()  # Wait for completion\n",
        "    \n",
        "    print(f\"‚úÖ Saved {len(df):,} rows to {table_id}\")\n",
        "    return table_id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save features and scores to BigQuery\n",
        "# Uncomment the lines below to actually save to BigQuery\n",
        "\n",
        "# Save wallet features\n",
        "# save_to_bigquery(features_to_save, ML_DATASET, 'wallet_features')\n",
        "\n",
        "# Save fraud scores\n",
        "# save_to_bigquery(scores_to_save, ML_DATASET, 'wallet_fraud_scores')\n",
        "\n",
        "print(\"üí° To save results to BigQuery, uncomment the save_to_bigquery() calls above.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Summary\n",
        "\n",
        "### Model Performance Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final summary\n",
        "print(\"=\"*60)\n",
        "print(\"ü§ñ ML FRAUD DETECTION SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nüìä DATA:\")\n",
        "print(f\"   Wallets analyzed: {len(wallet_addresses):,}\")\n",
        "print(f\"   Features computed: {len(feature_cols)}\")\n",
        "\n",
        "print(f\"\\nüîç MODELS:\")\n",
        "print(f\"   Isolation Forest anomalies: {iso_anomaly.sum():,} ({iso_anomaly.mean()*100:.1f}%)\")\n",
        "print(f\"   LOF anomalies: {lof_anomaly.sum():,} ({lof_anomaly.mean()*100:.1f}%)\")\n",
        "print(f\"   DBSCAN noise points: {dbscan_anomaly.sum():,} ({dbscan_anomaly.mean()*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nüéØ ENSEMBLE RESULTS:\")\n",
        "for cat in ['low', 'medium', 'high', 'critical']:\n",
        "    count = risk_counts.get(cat, 0)\n",
        "    pct = count / len(risk_categories) * 100\n",
        "    print(f\"   {cat.capitalize()}: {count:,} ({pct:.1f}%)\")\n",
        "\n",
        "print(f\"\\nüìÅ OUTPUTS:\")\n",
        "print(f\"   - ml_fraud_detection_results.png\")\n",
        "print(f\"   - wallet_features table (ready for BigQuery)\")\n",
        "print(f\"   - wallet_fraud_scores table (ready for BigQuery)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìö Next Steps\n",
        "\n",
        "1. **Hyperparameter Tuning**: Experiment with different model parameters\n",
        "2. **Feature Engineering**: Add more behavioral features (time-based, network-based)\n",
        "3. **Model Comparison**: Try other algorithms (One-Class SVM, Autoencoders)\n",
        "4. **Threshold Optimization**: Fine-tune risk category thresholds based on business requirements\n",
        "5. **Production Deployment**: Schedule regular model retraining via the `data_science` pipeline\n",
        "\n",
        "---\n",
        "\n",
        "### Key Findings\n",
        "\n",
        "- **Transaction Volume**: High-risk wallets typically have significantly higher transaction volumes\n",
        "- **Value Patterns**: Anomalous wallets often show unusual value distributions (very high or clustered values)\n",
        "- **Behavioral Indicators**: Concentrated counterparty interactions and unusual timing patterns are strong fraud indicators\n",
        "\n",
        "### Model Considerations\n",
        "\n",
        "- **Isolation Forest** works well for detecting isolated outliers\n",
        "- **LOF** captures local density anomalies\n",
        "- **DBSCAN** identifies wallets that don't fit any cluster pattern\n",
        "- **Ensemble** provides more robust detection by combining multiple perspectives\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
